{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb09d59b",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0003d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import nltk library\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed82920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords and add custom stopword if needed\n",
    "Stopwords=stopwords.words('english')\n",
    "customlistwords2exlude = []\n",
    "Stopwords.extend(customlistwords2exlude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba9ab34",
   "metadata": {},
   "source": [
    "# Load Data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac058f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for dataset (1157, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID           AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU        Julius NM   \n",
       "1          z13jhp0bxqncu512g22wvzkasxmvvzjaz04  ElNino Melendez   \n",
       "2          z13fwbwp1oujthgqj04chlngpvzmtt3r3dw           GsMega   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "2  2013-11-10T16:05:38            watch?v=vtaRGgvGtWQ   Check this out .﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  \n",
       "2      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train daset\n",
    "ads_ds=pd.read_csv(\"train.csv\")\n",
    "print(\"Shape for dataset\", ads_ds.shape)\n",
    "ads_ds.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ca7ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc</td>\n",
       "      <td>Jason Haddad</td>\n",
       "      <td>2013-11-26T02:55:11</td>\n",
       "      <td>Hey, check out my new website!! This site is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                   COMMENT_ID            AUTHOR  \\\n",
       "0   0  LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A       adam riyati   \n",
       "1   1  LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8  Evgeny Murashkin   \n",
       "2   2  LZQPQhLyRh9-wNRtlZDM90f1k0BrdVdJyN_YsaSwfxc      Jason Haddad   \n",
       "\n",
       "                  DATE                                            CONTENT  \n",
       "0  2013-11-07T12:37:15  Hey guys check out my new channel and our firs...  \n",
       "1  2013-11-08T17:34:21             just for test I have to say murdev.com  \n",
       "2  2013-11-26T02:55:11  Hey, check out my new website!! This site is a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load test dataset\n",
    "ads_ds_test = pd.read_csv(\"test.csv\")\n",
    "ads_ds_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2ad760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique comment - 0.9991356957649092\n"
     ]
    }
   ],
   "source": [
    "# Duplication in comments\n",
    "print(\"Number of unique comment -\", len(ads_ds.COMMENT_ID.unique())/len(ads_ds.COMMENT_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe77559a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s</td>\n",
       "      <td>janez novak</td>\n",
       "      <td>NaN</td>\n",
       "      <td>share and like this page to win a hand signed ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      COMMENT_ID       AUTHOR DATE  \\\n",
       "853  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s  janez novak  NaN   \n",
       "854  LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s  janez novak  NaN   \n",
       "\n",
       "                                               CONTENT  CLASS  \n",
       "853  share and like this page to win a hand signed ...      1  \n",
       "854  share and like this page to win a hand signed ...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Duplicate record found which same information so removed it from dataset\n",
    "comment_count = ads_ds.COMMENT_ID.value_counts()\n",
    "comment_count[comment_count>1]\n",
    "ads_ds[ads_ds.COMMENT_ID==\"LneaDw26bFvPh9xBHNw1btQoyP60ay_WWthtvXCx37s\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83a45c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for dataset (1156, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    COMMENT_ID           AUTHOR  \\\n",
       "0  LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU        Julius NM   \n",
       "1          z13jhp0bxqncu512g22wvzkasxmvvzjaz04  ElNino Melendez   \n",
       "\n",
       "                  DATE                                            CONTENT  \\\n",
       "0  2013-11-07T06:20:48  Huh, anyway check out this you[tube] channel: ...   \n",
       "1  2013-11-09T08:28:43   me shaking my sexy ass on my channel enjoy ^_^ ﻿   \n",
       "\n",
       "   CLASS  \n",
       "0      1  \n",
       "1      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicate row\n",
    "ads_ds=ads_ds.drop_duplicates()\n",
    "print(\"Shape for dataset\", ads_ds.shape)\n",
    "ads_ds.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfaa1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap came comment ID in train and test -  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o</td>\n",
       "      <td>Amir bassem</td>\n",
       "      <td>NaN</td>\n",
       "      <td>if u love rihanna subscribe me</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>_2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0</td>\n",
       "      <td>tyler sleetway</td>\n",
       "      <td>2013-10-05T00:57:25.078000</td>\n",
       "      <td>so beutiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID          AUTHOR  \\\n",
       "867   LneaDw26bFuH6iFsSrjlJLJIX3qD4R8-emuZ-aGUj0o     Amir bassem   \n",
       "1064  _2viQ_Qnc68fX3dYsfYuM-m4ELMJvxOQBmBOFHqGOk0  tyler sleetway   \n",
       "\n",
       "                            DATE                         CONTENT  CLASS  \n",
       "867                          NaN  if u love rihanna subscribe me      1  \n",
       "1064  2013-10-05T00:57:25.078000                     so beutiful      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot monthly \n",
    "ix = ads_ds.COMMENT_ID.isin(ads_ds_test.COMMENT_ID)\n",
    "print(\"Overlap came comment ID in train and test - \", sum(ix))\n",
    "ads_ds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7245e1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['M.E.S', 'Shadrach Grentz', 'Hidden Love', '5000palo', 'Louis Bryant',\n",
       "       'DanteBTV', 'Jacob Johnson', 'ThirdDegr3e', 'James Cook', 'Derek Moya',\n",
       "       ...\n",
       "       'Michael J. Cabose', 'Deepty Awasthy', 'Robert Petrea',\n",
       "       'BIANCA FLORENTINA MANTA', 'Victoria Morales',\n",
       "       'Fernando Luis Vega martinez', 'Ishfaq khan', 'Dana Matich',\n",
       "       'just a filthy kafir', 'faith jones'],\n",
       "      dtype='object', length=1094)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot Authors greater 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b80a3",
   "metadata": {},
   "source": [
    "# Data preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbf4856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE: balanced class dataset \n",
    "ads_ds[\"CLASS\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f57c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process dataset\n",
    "def preprocess_text(X):\n",
    "    ''' Function to pre-process dataset\n",
    "    '''\n",
    "    Punctuations = string.punctuation\n",
    "    for i in range(X.shape[0]):\n",
    "        Tokenize = nltk.tokenize.WhitespaceTokenizer()\n",
    "        Tokenized_comment =  Tokenize.tokenize(X[i])\n",
    "        Lemmmatized_comment = nltk.stem.WordNetLemmatizer()\n",
    "        stemm = PorterStemmer()\n",
    "        updated_comment = \"\"        \n",
    "        for word in Tokenized_comment:\n",
    "            if word not in Stopwords and word not in Punctuations:\n",
    "                word=word.lower()\n",
    "                updated_comment = updated_comment + \" \" + word\n",
    "        X[i]=updated_comment\n",
    "    return X\n",
    "                \n",
    "# create preprocessed train\n",
    "xtrain = preprocess_text(np.array(ads_ds[\"CONTENT\"]))\n",
    "ytrain = np.array(ads_ds[\"CLASS\"])\n",
    "\n",
    "# prepare test data\n",
    "xtest = preprocess_text(np.array(ads_ds_test[\"CONTENT\"]))\n",
    "idtest = np.array(ads_ds_test[\"ID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a771d9",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131900f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train-  (1156, 3162)\n",
      "Number of features in test -  (799, 3162)\n"
     ]
    }
   ],
   "source": [
    "# vectorize\n",
    "def vectorize(xtrain, xtest, mode=\"CountVectorizer\"):\n",
    "    ''' Function to create features\n",
    "    '''\n",
    "    if mode==\"CountVectorizer\":\n",
    "        feature=CountVectorizer()\n",
    "    else:\n",
    "        feature=TfidfVectorizer()\n",
    "    xtrainFeatures = feature.fit_transform(xtrain)\n",
    "    xtestFeatures = feature.transform(xtest)\n",
    "    return xtrainFeatures.toarray(),  xtestFeatures.toarray(), feature\n",
    "\n",
    "\n",
    "xtrainfeat, xtestfeat, feature = vectorize(xtrain, xtest)\n",
    "print(\"Number of features in train- \", xtrainfeat.shape)\n",
    "print(\"Number of features in test - \", xtestfeat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a28f10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' huh, anyway check you[tube] channel: kobyoshi02'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c4899c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '002',\n",
       " '018',\n",
       " '02',\n",
       " '034',\n",
       " '04',\n",
       " '047000',\n",
       " '05',\n",
       " '053012',\n",
       " '08',\n",
       " '09',\n",
       " '0d878a889c',\n",
       " '0laviqu2b',\n",
       " '10',\n",
       " '100',\n",
       " '10000000',\n",
       " '100007085325116',\n",
       " '10001',\n",
       " '100877300245414',\n",
       " '10200253113705769',\n",
       " '104999962146104962510',\n",
       " '10626048',\n",
       " '1073741828',\n",
       " '1073741830',\n",
       " '1073741943',\n",
       " '109',\n",
       " '11',\n",
       " '111719098841907',\n",
       " '12',\n",
       " '123',\n",
       " '124',\n",
       " '126',\n",
       " '128gb',\n",
       " '13',\n",
       " '1337',\n",
       " '134470083389909',\n",
       " '14',\n",
       " '1442646731',\n",
       " '1495323920744243',\n",
       " '1496241863981208',\n",
       " '1496273723978022',\n",
       " '1498561870415874',\n",
       " '14gkvdo',\n",
       " '15',\n",
       " '16',\n",
       " '161620527267482',\n",
       " '16gb',\n",
       " '17',\n",
       " '1727483389',\n",
       " '17yr',\n",
       " '18',\n",
       " '19',\n",
       " '19255',\n",
       " '1990',\n",
       " '19924',\n",
       " '1b',\n",
       " '1bi',\n",
       " '1billion',\n",
       " '1firo',\n",
       " '1hmvtx',\n",
       " '1k',\n",
       " '1m',\n",
       " '1m00s',\n",
       " '20',\n",
       " '200',\n",
       " '2004',\n",
       " '2009',\n",
       " '200k',\n",
       " '200mm',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2012430',\n",
       " '2012bitches',\n",
       " '2013',\n",
       " '2014',\n",
       " '201470069872822',\n",
       " '2015',\n",
       " '2017',\n",
       " '210',\n",
       " '2177367',\n",
       " '229508',\n",
       " '23',\n",
       " '23active',\n",
       " '23everydayimvaping',\n",
       " '23giraffebruuh',\n",
       " '23kinglothedancer',\n",
       " '23lmfao',\n",
       " '243a',\n",
       " '247',\n",
       " '25',\n",
       " '250',\n",
       " '25000',\n",
       " '251638183951',\n",
       " '25874',\n",
       " '26',\n",
       " '26032883',\n",
       " '26t22',\n",
       " '279',\n",
       " '28',\n",
       " '2asfn9shghk',\n",
       " '2b',\n",
       " '2billion',\n",
       " '2m19s',\n",
       " '2tggp3pv6l',\n",
       " '2x10',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '302703146601369',\n",
       " '313327',\n",
       " '320',\n",
       " '327568907427561',\n",
       " '32gb',\n",
       " '33',\n",
       " '333607726823679',\n",
       " '333608120156973',\n",
       " '33gxrf',\n",
       " '35',\n",
       " '360',\n",
       " '365',\n",
       " '385',\n",
       " '389088',\n",
       " '39',\n",
       " '390',\n",
       " '3d',\n",
       " '3m',\n",
       " '3m40s',\n",
       " '3rd',\n",
       " '40',\n",
       " '4000',\n",
       " '4000dollars',\n",
       " '41',\n",
       " '43',\n",
       " '4344749',\n",
       " '4477063',\n",
       " '447935454150',\n",
       " '4483179854075',\n",
       " '45',\n",
       " '46',\n",
       " '4604617',\n",
       " '476000',\n",
       " '482',\n",
       " '490',\n",
       " '4g',\n",
       " '4gb',\n",
       " '4m11s',\n",
       " '4netjobs',\n",
       " '4s',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '500k',\n",
       " '500m',\n",
       " '505b0232',\n",
       " '5094',\n",
       " '50k',\n",
       " '510',\n",
       " '515',\n",
       " '521',\n",
       " '5242575',\n",
       " '5277478',\n",
       " '5337555197',\n",
       " '53481',\n",
       " '543627485763966',\n",
       " '55',\n",
       " '5575096797',\n",
       " '55mm',\n",
       " '566',\n",
       " '57',\n",
       " '5800',\n",
       " '5af506e1',\n",
       " '5c',\n",
       " '5c2f',\n",
       " '5million',\n",
       " '5s',\n",
       " '5th',\n",
       " '5tu9gn1l310',\n",
       " '60',\n",
       " '600',\n",
       " '600m',\n",
       " '613000',\n",
       " '616375350',\n",
       " '6174122',\n",
       " '629',\n",
       " '6381501',\n",
       " '661',\n",
       " '666',\n",
       " '674732645945877',\n",
       " '694',\n",
       " '6th',\n",
       " '700',\n",
       " '710',\n",
       " '710000',\n",
       " '733634264',\n",
       " '733949243353321',\n",
       " '734237113324534',\n",
       " '74',\n",
       " '750',\n",
       " '775510675841486',\n",
       " '783',\n",
       " '79',\n",
       " '7in',\n",
       " '7k',\n",
       " '800',\n",
       " '82',\n",
       " '821',\n",
       " '824',\n",
       " '832000',\n",
       " '84',\n",
       " '85',\n",
       " '851',\n",
       " '857',\n",
       " '860',\n",
       " '868',\n",
       " '8692160',\n",
       " '87',\n",
       " '870',\n",
       " '88',\n",
       " '884',\n",
       " '898',\n",
       " '89___',\n",
       " '89c',\n",
       " '89iyec7nrwp5nytno5u7amhvmflutggl',\n",
       " '8a',\n",
       " '8bit',\n",
       " '90',\n",
       " '902099',\n",
       " '920',\n",
       " '9277547',\n",
       " '936868579660284',\n",
       " '937732262907249',\n",
       " '940',\n",
       " '969',\n",
       " '999999999',\n",
       " '9bzkp7q19f0',\n",
       " '9gag',\n",
       " '9nl',\n",
       " '_0f9fa8aa',\n",
       " '__',\n",
       " '______________________',\n",
       " '______________________________',\n",
       " '_chris_cz',\n",
       " '_self',\n",
       " '_trksid',\n",
       " 'a7',\n",
       " 'aa',\n",
       " 'aaas',\n",
       " 'aavpwj9',\n",
       " 'abbas',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'abominable',\n",
       " 'abomination',\n",
       " 'abonner',\n",
       " 'about',\n",
       " 'above',\n",
       " 'absolutely',\n",
       " 'abused',\n",
       " 'abusive',\n",
       " 'ac',\n",
       " 'acaer',\n",
       " 'acceptance',\n",
       " 'access',\n",
       " 'accidental',\n",
       " 'account',\n",
       " 'acidic',\n",
       " 'acquire',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'active',\n",
       " 'actor',\n",
       " 'actorid',\n",
       " 'actors',\n",
       " 'actresses',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'adele',\n",
       " 'adf',\n",
       " 'adhoc',\n",
       " 'admirable',\n",
       " 'admit',\n",
       " 'adore',\n",
       " 'adsense',\n",
       " 'advance',\n",
       " 'advertise',\n",
       " 'advertisements',\n",
       " 'advertisiments',\n",
       " 'affiliated',\n",
       " 'afflicted',\n",
       " 'afford',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'again',\n",
       " 'against',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'ahead',\n",
       " 'ahhh',\n",
       " 'aiiima',\n",
       " 'aimbwbfqbzg',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'airplane',\n",
       " 'aka',\n",
       " 'al',\n",
       " 'album',\n",
       " 'alcoholic',\n",
       " 'alex',\n",
       " 'alfred',\n",
       " 'ali',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allot',\n",
       " 'allow',\n",
       " 'allways',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'aloidia',\n",
       " 'alone',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'alternate',\n",
       " 'alvar',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazed',\n",
       " 'amazing',\n",
       " 'amazon',\n",
       " 'ambition',\n",
       " 'amendment',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'amiable',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anand',\n",
       " 'ancestors',\n",
       " 'and',\n",
       " 'anderson',\n",
       " 'android',\n",
       " 'angel',\n",
       " 'angels',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animals',\n",
       " 'animes',\n",
       " 'annoying',\n",
       " 'annoys',\n",
       " 'another',\n",
       " 'ans',\n",
       " 'anthem',\n",
       " 'antrobofficial',\n",
       " 'anxious',\n",
       " 'any',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'apocalypse',\n",
       " 'apologies',\n",
       " 'apostles',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'applause',\n",
       " 'apple',\n",
       " 'applied',\n",
       " 'applocker',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'apprecitate',\n",
       " 'apps',\n",
       " 'arbitrate',\n",
       " 'are',\n",
       " 'arive',\n",
       " 'arkglzjqup0',\n",
       " 'army',\n",
       " 'around',\n",
       " 'arrogant',\n",
       " 'arrowgance',\n",
       " 'art',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'artists',\n",
       " 'as',\n",
       " 'aseris',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'aslamu',\n",
       " 'aspx',\n",
       " 'ass',\n",
       " 'assume',\n",
       " 'astauand',\n",
       " 'aswell',\n",
       " 'at',\n",
       " 'attention',\n",
       " 'auburn',\n",
       " 'audio',\n",
       " 'auditioning',\n",
       " 'aunt',\n",
       " 'australia',\n",
       " 'authority',\n",
       " 'autotune',\n",
       " 'autotuned',\n",
       " 'avaaz',\n",
       " 'avicii',\n",
       " 'avoid',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'aways',\n",
       " 'awesom',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awsome',\n",
       " 'axeljonssons',\n",
       " 'ayyy',\n",
       " 'azerbaijan',\n",
       " 'b3',\n",
       " 'b5',\n",
       " 'b5t',\n",
       " 'b7b',\n",
       " 'b8l',\n",
       " 'ba',\n",
       " 'baba',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'bady',\n",
       " 'ball',\n",
       " 'band',\n",
       " 'bands',\n",
       " 'bang',\n",
       " 'bangers',\n",
       " 'bangladesh',\n",
       " 'barnesandnoble',\n",
       " 'bars',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'bass',\n",
       " 'bd3721315',\n",
       " 'bdp',\n",
       " 'be',\n",
       " 'beat',\n",
       " 'beatboxing',\n",
       " 'beaties',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'because',\n",
       " 'become',\n",
       " 'been',\n",
       " 'before',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'behavior',\n",
       " 'behind',\n",
       " 'behold',\n",
       " 'beibs',\n",
       " 'belgium',\n",
       " 'believe',\n",
       " 'believemefilm',\n",
       " 'believing',\n",
       " 'bella',\n",
       " 'belrus',\n",
       " 'beneath',\n",
       " 'bennett',\n",
       " 'berzerk',\n",
       " 'besloor',\n",
       " 'best',\n",
       " 'betfair',\n",
       " 'better',\n",
       " 'beutiful',\n",
       " 'bieber',\n",
       " 'big',\n",
       " 'bigboss286',\n",
       " 'bigelow',\n",
       " 'bigger',\n",
       " 'bighit',\n",
       " 'bikini',\n",
       " 'bil',\n",
       " 'bilion',\n",
       " 'billboard',\n",
       " 'billie',\n",
       " 'billion',\n",
       " 'bills',\n",
       " 'binbox',\n",
       " 'bing',\n",
       " 'birtgday',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'black',\n",
       " 'blanc',\n",
       " 'blank',\n",
       " 'blast',\n",
       " 'bleach',\n",
       " 'bless',\n",
       " 'block',\n",
       " 'blog',\n",
       " 'blogspot',\n",
       " 'blond',\n",
       " 'blow',\n",
       " 'blows',\n",
       " 'blue',\n",
       " 'boa',\n",
       " 'boaconic',\n",
       " 'body',\n",
       " 'bogdan',\n",
       " 'bones',\n",
       " 'bonus',\n",
       " 'boobs',\n",
       " 'book',\n",
       " 'bookmakers',\n",
       " 'boooobs',\n",
       " 'boost',\n",
       " 'border',\n",
       " 'bored',\n",
       " 'born',\n",
       " 'bots',\n",
       " 'bottom',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'br',\n",
       " 'brah',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'brazil',\n",
       " 'brazilian',\n",
       " 'break',\n",
       " 'breaks',\n",
       " 'brew',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brinkman',\n",
       " 'british',\n",
       " 'broken',\n",
       " 'brooooo',\n",
       " 'brother',\n",
       " 'brotherhood',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'browser',\n",
       " 'brt0u5',\n",
       " 'bs',\n",
       " 'btw',\n",
       " 'bubblews',\n",
       " 'buchmair',\n",
       " 'bucket',\n",
       " 'bulgaria',\n",
       " 'bumps',\n",
       " 'burda',\n",
       " 'burned',\n",
       " 'business',\n",
       " 'but',\n",
       " 'butalabs',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'butts',\n",
       " 'buy',\n",
       " 'buys',\n",
       " 'buzz',\n",
       " 'bxrosr',\n",
       " 'by',\n",
       " 'c3',\n",
       " 'c349',\n",
       " 'cabelo',\n",
       " 'cachebuster',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'cameraman',\n",
       " 'campid',\n",
       " 'can',\n",
       " 'canal',\n",
       " 'cant',\n",
       " 'canvas',\n",
       " 'capitalized',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'cares',\n",
       " 'caroline',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'catch',\n",
       " 'cause',\n",
       " 'caution',\n",
       " 'cd92db3f4',\n",
       " 'ce',\n",
       " 'cease',\n",
       " 'celeb',\n",
       " 'celebrated',\n",
       " 'celebrity',\n",
       " 'censor',\n",
       " 'cereal',\n",
       " 'certain',\n",
       " 'certification',\n",
       " 'cevxzvsjlk8',\n",
       " 'cge',\n",
       " 'chacking',\n",
       " 'chainise',\n",
       " 'challenge',\n",
       " 'challenges',\n",
       " 'champion',\n",
       " 'chance',\n",
       " 'chanel',\n",
       " 'chanell',\n",
       " 'change',\n",
       " 'changeable',\n",
       " 'chanicka',\n",
       " 'channel',\n",
       " 'channels',\n",
       " 'channnnnnelll',\n",
       " 'characterized',\n",
       " 'charley',\n",
       " 'charlie',\n",
       " 'chaste',\n",
       " 'chaîne',\n",
       " 'chcfcvzfzfbvzdr',\n",
       " 'cheat',\n",
       " 'cheating',\n",
       " 'cheats',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheetos',\n",
       " 'cheilith',\n",
       " 'chesture',\n",
       " 'chhanel',\n",
       " 'child',\n",
       " 'chillpal',\n",
       " 'chillstep',\n",
       " 'china',\n",
       " 'chinese',\n",
       " 'chiptunes',\n",
       " 'choice',\n",
       " 'chorenn',\n",
       " 'chose',\n",
       " 'chrck',\n",
       " 'christ',\n",
       " 'christians',\n",
       " 'christmas',\n",
       " 'chubby',\n",
       " 'chuck',\n",
       " 'cirus',\n",
       " 'cking',\n",
       " 'claiming',\n",
       " 'clap',\n",
       " 'class',\n",
       " 'classsic',\n",
       " 'clean',\n",
       " 'cleaning',\n",
       " 'click',\n",
       " 'clicked',\n",
       " 'clip',\n",
       " 'close',\n",
       " 'closer',\n",
       " 'clothes',\n",
       " 'clue',\n",
       " 'co',\n",
       " 'coby',\n",
       " 'cock',\n",
       " 'code',\n",
       " 'codytolleson',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'collaboration',\n",
       " 'collaborator',\n",
       " 'collection',\n",
       " 'collections',\n",
       " 'colors',\n",
       " 'colour',\n",
       " 'columbus',\n",
       " 'columns',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comeback',\n",
       " 'comentars',\n",
       " 'comes',\n",
       " 'comfort',\n",
       " 'comforter',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comment_id',\n",
       " 'commenting',\n",
       " 'comments',\n",
       " 'commercial',\n",
       " 'commit',\n",
       " 'community',\n",
       " 'company',\n",
       " 'competition',\n",
       " 'complaining',\n",
       " 'completely',\n",
       " 'composer',\n",
       " 'comprehend',\n",
       " 'computer',\n",
       " 'conceived',\n",
       " 'concert',\n",
       " 'concerts',\n",
       " 'conciliate',\n",
       " 'confessors',\n",
       " 'confidence',\n",
       " 'confirmed',\n",
       " 'confusing',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'congress',\n",
       " 'conhece',\n",
       " 'connected',\n",
       " 'conqueror',\n",
       " 'conscious',\n",
       " 'consolidating',\n",
       " 'constitution',\n",
       " 'constrictor',\n",
       " 'constructive',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'contest',\n",
       " 'continue',\n",
       " 'cook',\n",
       " 'cool',\n",
       " 'cooooooooooooolllllllllll',\n",
       " 'cope',\n",
       " 'copied',\n",
       " 'core',\n",
       " 'could',\n",
       " 'counsel',\n",
       " 'count',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'counts',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cover',\n",
       " 'covers',\n",
       " 'crabby',\n",
       " 'craft',\n",
       " 'crash',\n",
       " 'crashed',\n",
       " 'crazy',\n",
       " 'crdits',\n",
       " 'crea',\n",
       " 'creator',\n",
       " 'credit',\n",
       " 'crew',\n",
       " 'cried',\n",
       " 'criminal',\n",
       " 'criticism',\n",
       " 'critiquing',\n",
       " 'critisism',\n",
       " 'croatia',\n",
       " 'cross',\n",
       " 'crown',\n",
       " 'cruz',\n",
       " 'cry',\n",
       " 'cs',\n",
       " 'csgo',\n",
       " 'csrftoken',\n",
       " 'cup',\n",
       " 'curled',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'customid',\n",
       " 'cute',\n",
       " 'cutie',\n",
       " 'cuz',\n",
       " 'cxpzpgb',\n",
       " 'cyber',\n",
       " 'cypher',\n",
       " 'cyrine_ghorbel',\n",
       " 'cyrus',\n",
       " 'd15fb87973',\n",
       " 'd4aaacwk',\n",
       " 'd8',\n",
       " 'd9',\n",
       " 'd90',\n",
       " 'da',\n",
       " 'daaaaaaaaaaannng',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'dakoda',\n",
       " 'dakota',\n",
       " 'damn',\n",
       " 'damnnnnnnnn',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'dang',\n",
       " 'daniel',\n",
       " 'danke',\n",
       " 'dante',\n",
       " 'dark',\n",
       " 'dat',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dd',\n",
       " 'de',\n",
       " 'deaf',\n",
       " 'dealing',\n",
       " 'deals',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'deathly',\n",
       " 'deazy99',\n",
       " 'december',\n",
       " 'decent',\n",
       " 'decided',\n",
       " 'decio',\n",
       " 'deciocabelo',\n",
       " 'decoration',\n",
       " 'dedicated',\n",
       " 'deep',\n",
       " 'defectives',\n",
       " 'definitily',\n",
       " 'delete',\n",
       " 'delicious',\n",
       " 'delightful',\n",
       " 'demo',\n",
       " 'democracy',\n",
       " 'denis',\n",
       " 'derives',\n",
       " 'desenhos',\n",
       " 'deserve',\n",
       " 'deserves',\n",
       " 'designs',\n",
       " 'desire',\n",
       " 'destinyforever_',\n",
       " 'details',\n",
       " 'detective',\n",
       " 'df',\n",
       " 'diagnosed',\n",
       " 'dick',\n",
       " 'dickwad',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'die',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'direction',\n",
       " 'disabled',\n",
       " 'disappoint',\n",
       " 'disappointed',\n",
       " 'disclose',\n",
       " 'discount',\n",
       " 'discrimination',\n",
       " 'discusss',\n",
       " 'disguise',\n",
       " 'dislike',\n",
       " 'disliked',\n",
       " 'dislikes',\n",
       " 'dislikesssssssssssssssssssssssssssssssss',\n",
       " 'disorder',\n",
       " 'disposable',\n",
       " 'dissertation',\n",
       " 'divine',\n",
       " 'diving',\n",
       " 'diys',\n",
       " 'dj',\n",
       " 'dnckm',\n",
       " 'do',\n",
       " 'doctor',\n",
       " 'document',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'doh',\n",
       " 'dolacz',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'dom',\n",
       " 'dominate',\n",
       " 'don',\n",
       " 'donate',\n",
       " 'donating',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'dope',\n",
       " 'dosing',\n",
       " 'dot',\n",
       " 'doubt',\n",
       " 'down',\n",
       " 'download',\n",
       " 'downloading',\n",
       " 'downloads',\n",
       " 'drama',\n",
       " 'dreaddis',\n",
       " 'dream',\n",
       " 'dreamers',\n",
       " 'dreaming',\n",
       " 'dreams',\n",
       " 'dref',\n",
       " 'dress',\n",
       " 'dresses',\n",
       " 'dressprettyonce',\n",
       " 'drews',\n",
       " 'dribbleproshot',\n",
       " 'drink',\n",
       " 'drirathiel',\n",
       " 'driving',\n",
       " 'drone',\n",
       " 'drones',\n",
       " 'drop',\n",
       " 'drugs',\n",
       " 'drum',\n",
       " 'drums',\n",
       " 'drunk',\n",
       " 'du',\n",
       " 'dubstep',\n",
       " 'dude',\n",
       " 'dudes',\n",
       " 'due',\n",
       " 'duel',\n",
       " 'dumb',\n",
       " 'dundundunnn',\n",
       " 'dunno',\n",
       " 'during',\n",
       " 'duty',\n",
       " 'dvdscr',\n",
       " 'décio',\n",
       " 'earn',\n",
       " 'earned',\n",
       " 'earning',\n",
       " 'earns',\n",
       " 'earth',\n",
       " 'earthquake',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'easypromosapp',\n",
       " 'eat',\n",
       " 'eating',\n",
       " 'ebay',\n",
       " 'ebook',\n",
       " 'echa',\n",
       " 'ed',\n",
       " 'ede05ea397ca',\n",
       " 'edge',\n",
       " 'editor',\n",
       " 'edm',\n",
       " 'educated',\n",
       " 'eeccon',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'efforts',\n",
       " 'eggmode',\n",
       " 'egoistic',\n",
       " 'eh',\n",
       " 'ehi',\n",
       " 'either',\n",
       " 'ej',\n",
       " 'elephant',\n",
       " 'elevator',\n",
       " 'eliminate',\n",
       " 'elongate',\n",
       " 'else',\n",
       " 'em',\n",
       " 'ema',\n",
       " 'email',\n",
       " 'emerson_zanol',\n",
       " 'emi',\n",
       " 'eminem',\n",
       " 'eminems',\n",
       " 'emoji',\n",
       " 'emotions',\n",
       " 'emphasis',\n",
       " 'empire',\n",
       " 'empowering',\n",
       " 'en',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1ad9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_feature(xtrain):\n",
    "    regxFeatures = [] \n",
    "    for i in range(xtrain.shape[0]):\n",
    "        xtrain[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78291b9",
   "metadata": {},
   "source": [
    "# Modelling Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38a4e721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import tpe\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84846faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calROC(clf, x, y):\n",
    "    pred = clf.predict_proba(x)[:,1]\n",
    "    score = roc_auc_score(y, pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49ac4a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected : 199\n"
     ]
    }
   ],
   "source": [
    "# select feature with coverage\n",
    "ix=xtrainfeat.sum(axis=0)>xtrainfeat.shape[0]*0.01\n",
    "print(\"Number of features selected :\", sum(ix))\n",
    "xtrainfeat=xtrainfeat[:,ix]\n",
    "xtestfeat=xtestfeat[:, ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79e35d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test AUC -  0.9238916256157637\n",
      "Random Forest Overall AUC -  0.9983085604801891\n",
      "Random best param -  {'max_features': 'sqrt', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# Find optimal parameter for models\n",
    "model_score={}\n",
    "\n",
    "# Random Forest\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_param = {'n_estimators':[25, 50, 100], 'max_features':['sqrt', 'log2']}\n",
    "rf_clf=GridSearchCV(rf_clf, rf_param, cv=5)\n",
    "rf_clf = rf_clf.fit(xtrainfeat,ytrain )\n",
    "model_score[\"rf\"]= rf_clf.best_score_\n",
    "print(\"Random Forest Test AUC - \", model_score[\"rf\"])\n",
    "print(\"Random Forest Overall AUC - \", calROC(rf_clf.best_estimator_, xtrainfeat,ytrain))\n",
    "print(\"Random best param - \", rf_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "875498e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Test AUC -  0.92132407822063\n",
      "Gradient Boosting Overall AUC -  0.9968491325759276\n",
      "Gradient Boosting -  {'max_depth': 4, 'n_estimators': 250, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "gbm_clf = GradientBoostingClassifier()\n",
    "gbm_param = {'n_estimators':[200, 250, 300],'max_depth':[2, 3, 4], 'subsample':[0.8]}\n",
    "gbm_clf=GridSearchCV(gbm_clf, gbm_param, cv=5)\n",
    "gbm_clf = gbm_clf.fit(xtrainfeat,ytrain)\n",
    "model_score[\"gbm\"]= gbm_clf.best_score_\n",
    "print(\"Gradient Boosting Test AUC - \", model_score[\"gbm\"])\n",
    "print(\"Gradient Boosting Overall AUC - \", calROC(gbm_clf.best_estimator_, xtrainfeat,ytrain))\n",
    "print(\"Gradient Boosting - \", gbm_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36bc379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv', 'error_score', 'estimator__ccp_alpha', 'estimator__criterion', 'estimator__init', 'estimator__learning_rate', 'estimator__loss', 'estimator__max_depth', 'estimator__max_features', 'estimator__max_leaf_nodes', 'estimator__min_impurity_decrease', 'estimator__min_impurity_split', 'estimator__min_samples_leaf', 'estimator__min_samples_split', 'estimator__min_weight_fraction_leaf', 'estimator__n_estimators', 'estimator__n_iter_no_change', 'estimator__random_state', 'estimator__subsample', 'estimator__tol', 'estimator__validation_fraction', 'estimator__verbose', 'estimator__warm_start', 'estimator', 'n_jobs', 'param_grid', 'pre_dispatch', 'refit', 'return_train_score', 'scoring', 'verbose'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07112e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble overall auc :  0.9984522579969165\n"
     ]
    }
   ],
   "source": [
    "# overall ensemble\n",
    "rf_pred = rf_clf.best_estimator_.predict_proba(xtrainfeat)[:,1]\n",
    "gbm_pred = gbm_clf.best_estimator_.predict_proba(xtrainfeat)[:,1]\n",
    "pred_train = (rf_pred+gbm_pred)/2\n",
    "print(\"Ensemble overall auc : \", roc_auc_score(ytrain, pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622477fe",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24861b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.989982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.881586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.974604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.993025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      prob\n",
       "0   0  0.989982\n",
       "1   1  0.881586\n",
       "2   2  0.974604\n",
       "3   3  0.079330\n",
       "4   4  0.993025"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pred = rf_clf.best_estimator_.predict_proba(xtestfeat)[:,1]\n",
    "gbm_pred = gbm_clf.best_estimator_.predict_proba(xtestfeat)[:,1]\n",
    "\n",
    "pred = (rf_pred + gbm_pred)/2\n",
    "sub = []\n",
    "for ids, val in zip(idtest, pred):\n",
    "    sub.append([ids, val])\n",
    "sub = pd.DataFrame(sub)\n",
    "sub.columns = [\"ID\", \"prob\"]\n",
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ea56a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prob</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.989982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.881586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.974604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.079330</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.993025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID      prob  CLASS\n",
       "0   0  0.989982      1\n",
       "1   1  0.881586      1\n",
       "2   2  0.974604      1\n",
       "3   3  0.079330      0\n",
       "4   4  0.993025      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub['CLASS']=[0 if val<0.5 else 1 for val in sub['prob']]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4576eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"Ranvijay_sub_rf_gbm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf504b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
